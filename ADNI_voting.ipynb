{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9329021827000809\n",
      "Test accuracy: 0.6828087167070218\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Ensemble:\n",
    "    def __init__(self):\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load your dataset\n",
    "        data = pd.read_csv('amyloid.csv', na_values='--')\n",
    "        # Preprocess the data\n",
    "        data.dropna(inplace=True)  # Remove rows with missing values\n",
    "        X = data.iloc[:, 1:]  # Features\n",
    "        y = data['Amyloid']  # Target variable\n",
    "        \n",
    "        # Split the data\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n",
    "\n",
    "    @staticmethod\n",
    "    def __Classifiers__(name=None):\n",
    "        # Seed for reproducibility\n",
    "        random_state = 23\n",
    "\n",
    "        if name == 'decision_tree':\n",
    "            return DecisionTreeClassifier(random_state=random_state)\n",
    "        if name == 'kneighbors':\n",
    "            return KNeighborsClassifier()\n",
    "        if name == 'logistic_regression':\n",
    "            return LogisticRegression(random_state=random_state, solver='liblinear')\n",
    "        if name == 'svm_linear':\n",
    "            return SVC(kernel='linear', probability=True)\n",
    "        if name == 'svm_poly':\n",
    "            return SVC(kernel='poly', degree=3, probability=True)\n",
    "        if name == 'svm_rbf':\n",
    "            return SVC(kernel='rbf', probability=True)\n",
    "\n",
    "    def __VotingClassifier__(self):\n",
    "        # Instantiate classifiers including different SVM kernels\n",
    "        decision_tree = self.__Classifiers__('decision_tree')\n",
    "        knn = self.__Classifiers__('kneighbors')\n",
    "        logistic_regression = self.__Classifiers__('logistic_regression')\n",
    "        svm_linear = self.__Classifiers__('svm_linear')\n",
    "        svm_poly = self.__Classifiers__('svm_poly')\n",
    "        svm_rbf = self.__Classifiers__('svm_rbf')\n",
    "\n",
    "        # Voting Classifier initialization\n",
    "        vc = VotingClassifier(estimators=[\n",
    "            ('decision_tree', decision_tree),\n",
    "            ('knn', knn),\n",
    "            ('logistic_regression', logistic_regression),\n",
    "            ('svm_linear', svm_linear),\n",
    "            ('svm_poly', svm_poly),\n",
    "            ('svm_rbf', svm_rbf)\n",
    "        ], voting='soft')\n",
    "        \n",
    "        # Fitting the vc model\n",
    "        vc.fit(self.x_train, self.y_train)\n",
    "        \n",
    "        # Getting train and test accuracies from meta_model\n",
    "        y_pred_train = vc.predict(self.x_train)\n",
    "        y_pred = vc.predict(self.x_test)\n",
    "        \n",
    "        print(f\"Train accuracy: {accuracy_score(self.y_train, y_pred_train)}\")\n",
    "        print(f\"Test accuracy: {accuracy_score(self.y_test, y_pred)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ensemble = Ensemble()\n",
    "    ensemble.load_data()\n",
    "    ensemble.__VotingClassifier__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.12%\n",
      "Precision: 71.01%\n",
      "Recall: 73.62%\n",
      "F1 Score: 72.29%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC  # Import SVC for Support Vector Machine classifiers\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('amyloid.csv', na_values='--')\n",
    "data.dropna(inplace=True)  # Remove rows with missing values\n",
    "X = data.iloc[:, 1:]  # Features\n",
    "y = data['Amyloid']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set up the voting classifier with multiple classifiers\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = DecisionTreeClassifier(random_state=1)\n",
    "clf3 = RandomForestClassifier(random_state=1)\n",
    "clf4 = SVC(kernel='linear', probability=True, random_state=1)\n",
    "clf5 = SVC(kernel='rbf', probability=True, random_state=1)\n",
    "clf6 = SVC(kernel='poly', degree=3, probability=True, random_state=1)\n",
    "\n",
    "# Create the VotingClassifier with the classifiers\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('lr', clf1),\n",
    "    ('dt', clf2),\n",
    "    ('rf', clf3),\n",
    "    ('svm_linear', clf4),\n",
    "    ('svm_rbf', clf5),\n",
    "    ('svm_poly', clf6)\n",
    "], voting='soft')  # Using 'soft' voting to use the probability estimates for voting\n",
    "\n",
    "# Train the ensemble classifier\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "voting_pred = ensemble.predict(X_test)\n",
    "\n",
    "# Evaluate the metrics\n",
    "accuracy = accuracy_score(y_test, voting_pred)\n",
    "precision = precision_score(y_test, voting_pred, average='binary')\n",
    "recall = recall_score(y_test, voting_pred, average='binary')\n",
    "f1 = f1_score(y_test, voting_pred, average='binary')\n",
    "\n",
    "# Display the metrics as percentages\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.33%\n",
      "Precision: 73.01%\n",
      "Recall: 73.01%\n",
      "F1 Score: 73.01%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC  # Import SVC for Support Vector Machine classifiers\n",
    "from sklearn.preprocessing import StandardScaler  # Import the scaler\n",
    "from sklearn.pipeline import Pipeline  # Import Pipeline to handle scaling and model fitting\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('amyloid.csv', na_values='--')\n",
    "data.dropna(inplace=True)  # Remove rows with missing values\n",
    "X = data.iloc[:, 1:]  # Features\n",
    "y = data['Amyloid']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Pipeline for Logistic Regression with scaling\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # This will scale each feature to mean 0 and variance 1\n",
    "    ('logistic', LogisticRegression(C=10, solver='lbfgs', random_state=1, max_iter=1000))  # Increased max_iter\n",
    "])\n",
    "\n",
    "# Set up the voting classifier with multiple classifiers\n",
    "clf2 = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=1)\n",
    "clf3 = RandomForestClassifier(criterion='entropy', max_depth=None, max_features='log2', n_estimators=200, random_state=1)\n",
    "clf4 = SVC(kernel='linear', probability=True, random_state=1)  # Additional parameters could be tuned if needed\n",
    "clf5 = SVC(kernel='rbf', probability=True, random_state=1)     # Additional parameters could be tuned if needed\n",
    "clf6 = SVC(kernel='poly', C=1, gamma='scale', probability=True, random_state=1)\n",
    "clf7 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=1)\n",
    "clf8 = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Update the VotingClassifier with new classifiers\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('lr', lr_pipeline),\n",
    "    ('dt', clf2),\n",
    "    ('rf', clf3),\n",
    "    ('svm_linear', clf4),\n",
    "    ('svm_rbf', clf5),\n",
    "    ('svm_poly', clf6),\n",
    "    ('gb', clf7),\n",
    "    ('knn', clf8)\n",
    "], voting='soft')  # Continue using 'soft' voting\n",
    "# Train the ensemble classifier\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "voting_pred = ensemble.predict(X_test)\n",
    "\n",
    "# Evaluate the metrics\n",
    "accuracy = accuracy_score(y_test, voting_pred)\n",
    "precision = precision_score(y_test, voting_pred, average='binary')\n",
    "recall = recall_score(y_test, voting_pred, average='binary')\n",
    "f1 = f1_score(y_test, voting_pred, average='binary')\n",
    "\n",
    "# Display the metrics as percentages\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 53\u001b[0m\n\u001b[0;32m     47\u001b[0m knn_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_neighbors\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m],\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     50\u001b[0m }\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Create GridSearchCV for each model in the pipeline\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m lr_grid \u001b[38;5;241m=\u001b[39m GridSearchCV(\u001b[43mlr_pipeline\u001b[49m, lr_params, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     54\u001b[0m lr_grid\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest parameters for Logistic Regression: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr_grid\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Logistic Regression Grid\n",
    "lr_params = {\n",
    "    'logistic__C': [0.1, 1, 10],\n",
    "    'logistic__solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "# Decision Tree Grid\n",
    "dt_params = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Random Forest Grid\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# SVC Linear Grid\n",
    "svm_linear_params = {\n",
    "    'C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# SVC RBF Grid\n",
    "svm_rbf_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1]\n",
    "}\n",
    "\n",
    "# SVC Poly Grid\n",
    "svm_poly_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'degree': [2, 3, 4],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Gradient Boosting Grid\n",
    "gb_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# KNN Grid\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "# Create GridSearchCV for each model in the pipeline\n",
    "lr_grid = GridSearchCV(lr_pipeline, lr_params, cv=5, scoring='accuracy')\n",
    "lr_grid.fit(X_train, y_train)\n",
    "print(f'Best parameters for Logistic Regression: {lr_grid.best_params_}')\n",
    "\n",
    "# You can create similar GridSearchCV calls for the other classifiers\n",
    "# Note: Make sure to include fit and print best parameters for each model like shown above\n",
    "\n",
    "# Example for Decision Tree\n",
    "dt_grid = GridSearchCV(clf2, dt_params, cv=5, scoring='accuracy')\n",
    "dt_grid.fit(X_train, y_train)\n",
    "print(f'Best parameters for Decision Tree: {dt_grid.best_params_}')\n",
    "# Random Forest Grid Search\n",
    "rf_grid = GridSearchCV(clf3, rf_params, cv=5, scoring='accuracy')\n",
    "rf_grid.fit(X_train, y_train)\n",
    "print(f'Best parameters for Random Forest: {rf_grid.best_params_}')\n",
    "\n",
    "# SVC Linear Grid Search\n",
    "svm_linear_grid = GridSearchCV(clf4, svm_linear_params, cv=5, scoring='accuracy')\n",
    "svm_linear_grid.fit(X_train, y_train)\n",
    "print(f'Best parameters for SVM Linear: {svm_linear_grid.best_params_}')\n",
    "\n",
    "# SVC RBF Grid Search\n",
    "svm_rbf_grid = GridSearchCV(clf5, svm_rbf_params, cv=5, scoring='accuracy')\n",
    "svm_rbf_grid.fit(X_train, y_train)\n",
    "print(f'Best parameters for SVM RBF: {svm_rbf_grid.best_params_}')\n",
    "\n",
    "# # SVC Poly Grid Search\n",
    "# svm_poly_grid = GridSearchCV(clf6, svm_poly_params, cv=5, scoring='accuracy')\n",
    "# svm_poly_grid.fit(X_train, y_train)\n",
    "# print(f'Best parameters for SVM Poly: {svm_poly_grid.best_params_}')\n",
    "\n",
    "# Gradient Boosting Grid Search\n",
    "gb_grid = GridSearchCV(clf7, gb_params, cv=5, scoring='accuracy')\n",
    "gb_grid.fit(X_train, y_train)\n",
    "print(f'Best parameters for Gradient Boosting: {gb_grid.best_params_}')\n",
    "\n",
    "# KNN Grid Search\n",
    "knn_grid = GridSearchCV(clf8, knn_params, cv=5, scoring='accuracy')\n",
    "knn_grid.fit(X_train, y_train)\n",
    "print(f'Best parameters for KNN: {knn_grid.best_params_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.30%\n",
      "Precision: 71.81%\n",
      "Recall: 65.64%\n",
      "F1 Score: 68.59%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC  # Import SVC for Support Vector Machine classifiers\n",
    "from sklearn.preprocessing import StandardScaler  # Import the scaler\n",
    "from sklearn.pipeline import Pipeline  # Import Pipeline to handle scaling and model fitting\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('amyloid.csv', na_values='--')\n",
    "data.dropna(inplace=True)  # Remove rows with missing values\n",
    "X = data.iloc[:, 1:]  # Features\n",
    "y = data['Amyloid']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Pipeline for Logistic Regression with scaling\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # This will scale each feature to mean 0 and variance 1\n",
    "    ('logistic', LogisticRegression(C=10, solver='lbfgs', random_state=1, max_iter=1000))  # Increased max_iter\n",
    "])\n",
    "\n",
    "# Set up the voting classifier with multiple classifiers\n",
    "clf2 = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=1)\n",
    "clf3 = RandomForestClassifier(criterion='entropy', max_depth=None, max_features='log2', n_estimators=200, random_state=1)\n",
    "clf4 = SVC(kernel='linear', probability=True, random_state=1)  # Additional parameters could be tuned if needed\n",
    "clf5 = SVC(kernel='rbf', probability=True, random_state=1)     # Additional parameters could be tuned if needed\n",
    "clf6 = SVC(kernel='poly', C=1, gamma='scale', probability=True, random_state=1)\n",
    "clf7 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=1)\n",
    "clf8 = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Update the VotingClassifier with new classifiers\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('lr', lr_pipeline),\n",
    "    ('dt', clf2),\n",
    "    ('rf', clf3),\n",
    "    ('svm_linear', clf4),\n",
    "    ('svm_rbf', clf5),\n",
    "    ('svm_poly', clf6),\n",
    "    ('gb', clf7),\n",
    "    ('knn', clf8)\n",
    "], voting='hard')  # Continue using 'soft' voting\n",
    "# Train the ensemble classifier\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "voting_pred = ensemble.predict(X_test)\n",
    "\n",
    "# Evaluate the metrics\n",
    "accuracy = accuracy_score(y_test, voting_pred)\n",
    "precision = precision_score(y_test, voting_pred, average='binary')\n",
    "recall = recall_score(y_test, voting_pred, average='binary')\n",
    "f1 = f1_score(y_test, voting_pred, average='binary')\n",
    "\n",
    "# Display the metrics as percentages\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
